# Copyright 2023 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""evaluate the network generated by the algorithm"""

import os
import pickle
import multiprocessing as mp
import stat
import logging as logger
import mindspore.nn as nn
import mindspore as ms
from mindspore import context
from mindspore.communication.management import init
from mindspore.train.callback import LossMonitor, TimeMonitor
from mindspore.context import ParallelMode
from data.get_data import get_ri_for_search
from model_utils.config import config
from model_utils.device_adapter import get_device_num, get_device_id
import numpy as np


logger.basicConfig(level=logger.INFO, format='%(asctime)s - %(levelname)s: %(message)s')


def train_process(individual, pipe):
    learning_rate = config.learning_rate
    cfg = config

    context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target)
    device_num = get_device_num()
    if cfg.device_target == "Ascend":
        device_id = get_device_id()
        context.set_context(device_id=device_id)
        if device_num > 1:
            context.reset_auto_parallel_context()
            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,
                                              gradients_mean=True)
            init()
    elif cfg.device_target == "GPU":
        context.set_context(enable_graph_kernel=True)
        if device_num > 1:
            init()
            context.reset_auto_parallel_context()
            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,
                                              gradients_mean=True)

    train_set, valid_set = get_ri_for_search(config.data_path, batch_size=cfg.batch_size)

    try:
        net = Model(individual)
    except ValueError:
        result_dict = {'acc': 0.0, 'loss': 100.0, 'parameters': 1e10}
        pipe.send(result_dict)
        return

    parameters, loss_scale_manager = 0, None
    for p in net.trainable_params():
        parameters += np.cumprod(p.shape)[-1]

    opt = nn.Adam(net.trainable_params(), learning_rate=learning_rate)
    loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')

    model = ms.Model(net, loss_fn=loss, optimizer=opt, metrics={'acc', 'loss'},
                     amp_level="O2", keep_batchnorm_fp32=False, loss_scale_manager=loss_scale_manager)

    time_cb = TimeMonitor(data_size=128)
    loss_cb = LossMonitor()

    cbs = [time_cb, loss_cb]


    try:
        model.train(cfg.epoch_size, train_set, callbacks=cbs)
        result_dict = model.eval(valid_set)
    except ValueError:
        result_dict = {'acc': 0.0, 'loss': 100.0}
    except RuntimeError:
        result_dict = {'acc': 0.0, 'loss': 100.0}
    result_dict['parameters'] = parameters
    logger.info(result_dict)

    pipe.send(result_dict)


class Evaluate:
    def __init__(self, pops, input_shape, epochs, batch_size, train_data_length, validate_data_length, is_valid=True):
        self._pops = pops
        self._input_shape = input_shape
        self._epochs = epochs
        self._batch_size = batch_size
        self._train_data_length = train_data_length
        self._validate_data_length = validate_data_length
        self._epochs_offset = 0
        self._is_valid = is_valid

    def parse_populations(self, epochs_offset):
        history_best_score = 0
        self._epochs_offset = max(0, epochs_offset)
        self._epochs_offset = int(epochs_offset / 5)
        self._epochs += self._epochs_offset
        for population_index in range(self._pops.get_populations_size()):
            individual = self._pops.get_populations_at(population_index)
            try:
                mean_, std_, num_connections_, new_best = \
                    self._parse_individual(individual, population_index, history_best_score)
            except ValueError:
                mean_ = 0.0
                std_ = 0.0
                num_connections_ = 1000000
                new_best = history_best_score
                logger.info('error')
            individual.set_performance_score(mean_, std_, num_connections_)

            flags, modes = os.O_RDWR | os.O_CREAT, stat.S_IWUSR | stat.S_IRUSR

            fd = os.open(os.path.join(config.individual_history_path,
                                      '{}-{}.dat'.format(epochs_offset, population_index)), flags, modes)
            with os.fdopen(fd, 'wb') as f:
                pickle.dump(individual, f)
            history_best_score = new_best

    def _parse_individual(self, individual, index, history_best_score):
        logger.info('the current epochs is %d | %d/%d', self._epochs, index, self._pops.get_populations_size())
        try:
            conn1, conn2 = mp.Pipe()
            process = mp.Process(target=train_process, args=(individual, conn1))
            process.start()
            result_dict = conn2.recv()
            process.join()
            process.close()
            individual.training_history['test_acc'] = result_dict['acc']
            individual.training_history['test_loss'] = result_dict['loss']
            if result_dict['acc'] > history_best_score:
                history_best_score = result_dict['acc']
            num_connections = result_dict['parameters']
        except ValueError:
            individual.training_history['test_acc'] = 0.0
            individual.training_history['test_loss'] = 0.0
            num_connections = 1e10

        return [individual.training_history['test_acc'],
                np.std([individual.training_history['test_acc']]),
                num_connections, history_best_score]


class Model(nn.Cell):
    def __init__(self,
                 individual):
        super(Model, self).__init__()
        layers = []
        current_channel = 1
        flatten_flag = False
        for layer_index in range(individual.get_layer_size()):
            current_layer = individual.get_layer_at(layer_index)
            if current_layer.type == 1:
                conv_ = nn.Conv2d(current_channel, current_layer.feature_size, current_layer.filter_size,
                                  pad_mode='same',
                                  weight_init=current_layer.get_initializer())
                layers.append(conv_)
                current_channel = current_layer.feature_size
                # batch norm and active function
                if current_layer.order > 0.5:
                    # batch norm
                    if current_layer.batch_norm > 0.5:
                        bn_ = nn.BatchNorm2d(current_channel)
                        layers.append(bn_)
                    # active function
                    active_function = current_layer.get_active_fn()
                    layers.append(active_function())
                    if current_layer.get_active_fn_name() == 'crelu':
                        current_channel = current_channel * 2
                else:
                    # active function
                    active_function = current_layer.get_active_fn()
                    layers.append(active_function())
                    if current_layer.get_active_fn_name() == 'crelu':
                        current_channel = current_channel * 2
                    # batch norm
                    if current_layer.batch_norm > 0.5:
                        bn_ = nn.BatchNorm2d(current_channel)
                        layers.append(bn_)
                # dropout
                if current_layer.dropout_rate >= 0.1:
                    dropout_ = nn.Dropout(keep_prob=1.0 - current_layer.get_dropout_rate())
                    layers.append(dropout_)
            elif current_layer.type == 2:  # Pool
                pool_ = nn.MaxPool2d(current_layer.kernel_size, stride=2)
                layers.append(pool_)
                # dropout
                if current_layer.dropout_rate >= 0.1:
                    dropout_ = nn.Dropout(keep_prob=1.0 - current_layer.get_dropout_rate())
                    layers.append(dropout_)
            elif current_layer.type == 3:  # Full
                if not flatten_flag:
                    layers.append(nn.Flatten())
                    current_channel = current_layer.input_size[0] * current_layer.input_size[1] * current_channel
                    flatten_flag = True
                initializer_fn = current_layer.get_initializer()
                full_ = nn.Dense(current_channel, current_layer.hidden_num, weight_init=initializer_fn)
                layers.append(full_)
                current_channel = current_layer.hidden_num
                if not layer_index == individual.get_layer_size() - 1:
                    # batch norm and active function
                    if current_layer.order > 0.5:
                        # batch norm
                        if current_layer.batch_norm > 0.5:
                            bn_ = nn.BatchNorm1d(current_channel)
                            layers.append(bn_)
                        # active function
                        active_function = current_layer.get_active_fn()
                        layers.append(active_function())
                        if current_layer.get_active_fn_name() == 'crelu':
                            current_channel = current_channel * 2
                    else:
                        # active function
                        active_function = current_layer.get_active_fn()
                        layers.append(active_function())
                        if current_layer.get_active_fn_name() == 'crelu':
                            current_channel = current_channel * 2
                        # batch norm
                        if current_layer.batch_norm > 0.5:
                            bn_ = nn.BatchNorm1d(current_channel)
                            layers.append(bn_)
                    # dropout
                    if current_layer.dropout_rate >= 0.1:
                        dropout_ = nn.Dropout(keep_prob=1.0 - current_layer.get_dropout_rate())
                        layers.append(dropout_)
            else:
                raise NameError('No unit with type value: {}'.format(current_layer.type))

        self.build_block = nn.SequentialCell(layers)

    def construct(self, x):
        return self.build_block(x)
